#!/usr/bin/env python
import argparse
import colorama
import hashlib
import multiprocessing
import os
import pathlib
import pickle
import platform
import shlex
import shutil
import subprocess
import traceback

from colorama import Fore, Style

# Initialize colorama
colorama.init()

# Colors
class colors:
    if platform.system() == 'Windows':
        OKBLUE = Fore.BLUE
        OKCYAN = Fore.CYAN
        OKGREEN = Fore.GREEN
        WARNING = Fore.YELLOW
        FAIL = Fore.RED
        RESET = Style.RESET_ALL
        BOLD = Style.BRIGHT
    else:
        OKBLUE = '\033[94m'
        OKCYAN = '\033[96m'
        OKGREEN = '\033[92m'
        WARNING = '\033[93m'
        FAIL = '\033[91m'
        RESET = '\033[0m'
        BOLD = '\033[1m'

# Generate message
def message(type, color, message):
    return colors.BOLD + color + type + ': ' + colors.RESET + message

# Assert that a file exists
def assert_file(file_path):
    if not os.path.exists(file_path):
        msg = 'file ' + file_path + ' does not exist'
        print(message('fatal error', colors.FAIL, msg))
        exit(-1)

# Error exceptions
class InvalidInput(Exception):
    def __init__(self, message):
        self.message = message

    def __str__(self):
        return self.message

# Asserting a specific type
def assert_type(var, vtype, msg):
    if not isinstance(var, vtype):
        raise InvalidInput(msg)

# Script class to encapsulate script and its options
class Script:
    def __init__(self, pathdir):
        self.commands = []
        self._cwd = pathdir
        self._pathdir = pathdir

    def set_cwd(self, cwd):
        self._cwd = os.path.join(self._pathdir, cwd)
        self._cwd = os.path.normpath(self._cwd)

        if not os.path.exists(self._cwd):
            msg = 'directory ' + self._cwd + ' does not exist'
            raise InvalidInput(msg)

    def __repr__(self):
        return 'Script {\n' \
            + '\tcommands: ' + str(self.commands) + '\n' \
            + '\tcwd: ' + str(self._cwd) + '\n' \
            + '}'

# Build and Target structures
class Build:
    # Constants
    DEFAULT_COMPILER = 'g++'

    def __init__(self, code, path, exec=None):
        # TODO: prevent pickle from serializing target...
        self.exec = exec # None if not executable/not linked
        self.path = path
        self.sources = set()
        self.includes = set()
        self.libraries = set()
        self.flags = set()
        self.macros = {}
        self.compiler = Build.DEFAULT_COMPILER

        self.requires = [] # Build dependencies

        # Configurations
        # TODO: list of config attribute names for easy copy
        self.linking = True
        self.object_format = None
        self.object_dir = None

        # Fields to be filled by program
        self._cc_dependencies = None
        self._cc_dir = None
        self._cc_includes = None
        self._cc_sources = None

        # Encode unique identifier
        self.code = code

    # Comparison to cached build
    def __eq__(self, other):
        if not isinstance(other, Build):
            return False

        # Check if all fields are equal
        # TODO: can we ensuret hat we can one line all this?
        # NOTE: we do not check for requires
        if not self.exec == other.exec:
            return False

        if not self.path == other.path:
            return False

        if not self.sources == other.sources:
            return False

        if not self.includes == other.includes:
            return False

        if not self.libraries == other.libraries:
            return False

        if not self.flags == other.flags:
            return False

        if not self.macros == other.macros:
            return False

        if not self.compiler == other.compiler:
            return False

        return self.code == other.code

    def define(self, macro_map):
        # Make sure macro_map is a dictionary
        if not isinstance(macro_map, dict):
            raise InvalidInput('macro_map must be a dictionary')

        # Make sure all keys are strings
        for key in macro_map.keys():
            if not isinstance(key, str):
                raise InvalidInput('macro_map keys must be strings')

        self.macros.update(macro_map)

    # Editing the configuration
    def config_key(self, key, value):
        if not isinstance(key, str):
            raise InvalidInput('keys must be strings')

        if key == 'linking':
            assert_type(value, bool, 'linking must be a boolean')
            self.linking = value
        elif key == 'object_format':
            assert_type(value, str, 'object_format must be a string')
            self.object_format = value
        elif key == 'object_dir':
            assert_type(value, str, 'object_dir must be a string')
            self.object_dir = value
        else:
            raise InvalidInput('invalid configuration key "' + key + '"')

    def config(self, configs):
        assert_type(configs, dict, 'configuration must be a dictionary')
        for key, value in configs.items():
            self.config_key(key, value)

    # Add build dependencies
    def require(self, *builds):
        for build in builds:
            if isinstance(build, list):
                self.requires.extend(build)
            elif isinstance(build, Build):
                self.requires.append(build)
            else:
                raise InvalidInput('build must be a Build or list of Builds')

    def set_compiler(self, compiler):
        self.compiler = compiler

    def add_sources(self, *sources):
        for source in sources:
            if isinstance(source, list):
                self.sources.update(source)
                # self.sources.extend(source)
            elif isinstance(source, str):
                self.sources.add(source)
                # self.sources.append(source)
            else:
                raise InvalidInput('source must be a string or list of strings')

    def add_includes(self, *includes):
        for include in includes:
            if isinstance(include, list):
                self.includes.update(include)
            elif isinstance(include, str):
                self.includes.add(include)
            else:
                raise InvalidInput('include must be a string or list of strings')

    def add_libraries(self, *libraries):
        for library in libraries:
            if isinstance(library, list):
                self.libraries.update(library)
            elif isinstance(library, str):
                self.libraries.add(library)
            else:
                raise InvalidInput('library must be a string or list of strings')

    def set_flags(self, flags):
        if isinstance(flags, list):
            # Dont split here...
            self.flags = flags
        else:
            self.flags = shlex.split(flags)

    def add_flags(self, flags):
        self.flags += ' ' + flags

class Target:
    all = {}

    def __init__(self, name, path):
        self.name = name
        self.path = path
        self.builds = {}

        # Add to all targets
        Target.all[name] = self
        self.name = name
        self.path = path
        self.builds = {}

        # TODO: multiple post-builds and install scripts..
        self.post_builds = {}
        self.install_scripts = []

        # Add to all targets
        Target.all[name] = self

    def new_build(self, mode):
        if not isinstance(mode, str):
            raise InvalidInput('mode must be a string')

        if mode in self.builds:
            msg = 'build for mode ' + mode + ' already exists for target ' + self.name
            print(message('warning', colors.WARNING, msg))

        build = Build(self.name + '_' + mode, self.path, self.name)
        self.builds[mode] = build
        return build

    def set_post_build(self, script):
        mode = 'default'
        if isinstance(script, str):
            cmd = script
            script = Script(os.path.dirname(self.path))
            script.commands.append(cmd)
        elif isinstance(script, Script):
            pass
        elif isinstance(script, tuple) \
                and isinstance(script[0], str) \
                and isinstance(script[1], str):
            mode = script[0]
            cmd = script[1]
            script = Script(os.path.dirname(self.path))
            script.commands.append(cmd)
        elif isinstance(script, tuple) \
                and isinstance(script[0], str) \
                and isinstance(script[1], Script):
            mode = script[0]
            script = script[1]
        else:
            raise InvalidInput(
                'provided argument must be of' + \
                ' the forms: str, Script, (str, str), (str, Script)'
            )

        assert(isinstance(script, Script))
        self.post_builds[mode] = script
    
    def set_post_builds(self, *scripts):
        for script in scripts:
            self.set_post_build(script)

    def on_install(self, *scripts):
        self.install_scripts.extend(scripts)

def smake_import(file, path):
    file = os.path.join(path, file)
    # Make sure path is a string
    if not isinstance(file, str):
        raise InvalidInput('path must be a string')

    # Make sure path exists
    if not os.path.exists(file):
        raise InvalidInput('file' + file + ' does not exist')

    # Make sure path is a file
    if not os.path.isfile(file):
        raise InvalidInput('file' + file + ' is not a file')

    # Exec and extract variables
    global_vars = globals()

    global_vars['__file__'] = file
    exec(open(file).read(), global_vars)

# TODO: How useless is this?
def localize(sources, list):
    '''Localize a list of sources to a given directory'''
    localized = []
    for source in sources:
        localized.append(os.path.join(list, source))
    return localized

def evaluate_file(path):
    # All variables in a single evaluation context
    path = os.path.abspath(path)
    pathdir = os.path.dirname(path)

    variables = {
            # TODO: these functions should go in a detail namespace or something
            'target': lambda name: Target(name, path),
            'script': lambda: Script(pathdir),
            'smake_import': lambda file: smake_import(file, pathdir),
            'proxy_build': lambda name: Build('proxy_' + name, path, name),
            'localize': localize,
    }

    # Open the file
    file = open(path, 'r')
    content = file.read()

    try:
        compiled = compile(content, path, 'exec')
        exec(compiled, globals(), variables)
    except Exception as e:
        print(message('error', colors.FAIL, str(e)))
        traceback.print_tb(e.__traceback__)
        exit(-1)

# Read all .smake files from this directory or any subdirectory
def load_smakes():
    smake_files = []
    for root, _, files in os.walk('.'):
        for file in files:
            if file.endswith('.smake'):
                smake_file = os.path.join(root, file)
                smake_files.append(smake_file)

    for smake_file in smake_files:
        evaluate_file(smake_file)

    # Warn if no smake files found
    if len(smake_files) == 0:
        print(message('warning', colors.WARNING, 'no smake files found'))

def list_targets(targets):
    # Return if no targets
    if len(targets) == 0:
        return

    # Compute padding
    maxlen = 0
    for target in targets:
        if len(target) > maxlen:
            maxlen = len(target)

    maxlen += 5

    # Header message
    fmt = colors.BOLD + colors.OKCYAN + '{:<' + str(maxlen) + '}' \
        + colors.OKBLUE + '{}' + colors.RESET

    print(fmt.format('Target', 'Build modes'))

    # Print the targets
    for name in targets:
        target = targets[name]

        modes = list(target.builds.keys())
        modes = ', '.join(modes)

        fmt = colors.OKCYAN + '{:<' + str(maxlen) + '}' + \
            colors.OKBLUE + '{}' + colors.RESET
        print(fmt.format(target.name, modes))

def __dependency_thread_init__(_counter, _total):
    global counter, total

    counter = _counter
    total = _total

def get_dependencies(packed):
    global counter, total

    source, cmd = packed
    proc = subprocess.Popen(cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )

    proc.wait()
    out, err = proc.communicate()

    out = out.decode('utf-8')
    err = err.decode('utf-8')

    if proc.returncode != 0:
        with counter.get_lock():
            print(message('error', colors.FAIL, 'failed to get dependencies for ' + source))
            print(err)
        
            msg = colors.BOLD + f'[{counter.value}/{total}]' + colors.RESET
            msg += ' Generating dependencies...'
            print(msg, end='\r')

            counter.value += 1

        return None

    with counter.get_lock():
        msg = colors.BOLD + f'[{counter.value}/{total}]' + colors.RESET
        msg += ' Generating dependencies...'
        print(msg, end='\r')
        counter.value += 1
    
    return out.split(' ')[1:]

# Preprocess a particular build configuration
#   There are quite a few issues when trying to cache
#   the dependencies. For example, we need to update the file
#   if the smake file changes, if the source file changes, if any
#   of the dependencies change, etc. This is a lot of work, and
#   it's not clear if it's worth it. For now, we skip caching
#   Maybe we can multithread this?
def preprocess_build(build, threads):
    # TODO: expand sources (if regex) and make sure they exist
    build._cc_dir = os.path.dirname(build.path)
    fixed_sources = [
        os.path.relpath(os.path.join(build._cc_dir, source))
        for source in build.sources
    ]

    faulty_sources = False

    # TODO: kernel for executing dependency commands
    sources = []
    for source in fixed_sources:
        p = pathlib.Path('.')
        glob = p.glob(source)

        count = 0
        for g in glob:
            # Skip if directory (no recursive globbing)
            if g.is_dir():
                continue

            # Get absolute path
            g = os.path.abspath(g)
            sources.append(str(g))
            count += 1

        if count == 0:
            print(message('error', colors.FAIL, 'could not find source file(s) ' + source))
            faulty_sources = True

    if faulty_sources:
        return False

    # Compile all includes
    cc_includes = []
    for include in build.includes:
        abs_include = os.path.join(build._cc_dir, include)
        abs_include = os.path.abspath(abs_include)

        if not os.path.exists(abs_include):
            print(message('warning', colors.WARNING, 'could not locate include directory ' + include))
        else:
            cc_includes.append(abs_include)

    cc_includes = ['-I' + include for include in cc_includes]

    # Generate the commands for generation dependencies
    cc_dep_commands = []
    for source in sources:
        cmd = [build.compiler, '-MM', source]
        cmd.extend(cc_includes)

        cc_dep_commands.append((source, cmd))

    # Concurrently get dependencies
    counter = multiprocessing.Value('i', 1)
    with multiprocessing.Pool(threads,
            initializer=__dependency_thread_init__,
            initargs=(counter, len(cc_dep_commands))) as pool:
        rets = pool.map(get_dependencies, cc_dep_commands)

    # Check for errors
    for ret in rets:
        if ret == None:
            print()
            print(message('error', colors.FAIL,
                'Failed to generate dependencies'
            ))

            return False

    # Process the dependencies
    dependency_list = []
    source_info = {}

    for i in range(len(sources)):
        source = sources[i]
        deps = rets[i]
        indices = []

        for dep in deps:
            dep = dep.strip()
            if dep in ['', '\\', ':']:
                continue

            dep = os.path.normpath(dep)
            if dep in dependency_list:
                index = dependency_list.index(dep)
            else:
                index = len(dependency_list)
                dependency_list.append(dep)

            indices.append(index)

        source_info[source] = indices

    # Forward information to the build object
    build._cc_includes = cc_includes
    build._cc_dependencies = dependency_list
    build._cc_sources = source_info
    
# Get modification time for each dependency
def get_dependency_mtime(dependencies):
    dependency_mtime = []
    for dependency in dependencies:
        mtime = os.path.getmtime(dependency)
        dependency_mtime.append(mtime)

    return dependency_mtime

# Generate build commands
def generate_commands(build):
    # Create the build directories
    # TODO: assist method
    smake_dir = os.path.join(build._cc_dir, '.smake')
    all_builds_dir = os.path.join(smake_dir, 'builds')

    build_name = build.code

    build_dir = os.path.join(all_builds_dir, build_name)
    if not build.object_dir is None:
        build_dir = build.object_dir

    os.makedirs(build_dir, exist_ok=True)

    # Compilation commands
    command_map = {}
    hashed_sources = {}

    build_outputs = []
    for source in build._cc_sources:
        # Object name map
        filename = os.path.basename(source)
        index_of_dot = filename.index('.')
        basename = filename
        if index_of_dot != -1:
            basename = filename[:index_of_dot]

        naming_vars = {
                'FILENAME': filename,
                'BASENAME': basename
        }
        
        output = None
        if not build.object_format is None:
            output = build.object_format
            for var in naming_vars:
                output = output.replace('${' + var + '}', naming_vars[var])
        else:
            # Create object output name
            output = source.replace('/', '_') + '.o'

            # Hash each output to a shorter name
            m = hashlib.md5()
            m.update(output.encode('utf-8'))
            m = m.hexdigest()[:10]

            if m in hashed_sources:
                hashed_sources[m] += 1
            else:
                hashed_sources[m] = 1

            m += '{:04d}'.format(hashed_sources[m])

            output = m + '.o'

        # Keep in the designated build output directory
        # unless otherwise specified
        output = os.path.join(build_dir, output)

        build_outputs.append(output)

        cmd_compile = [build.compiler, '-c', source, '-o', output]
        cmd_compile += build.flags
        cmd_compile += build._cc_includes

        for macro, value in build.macros.items():
            macro = '-D' + macro

            if value is None:
                pass # This is actually OK, define without value
            if isinstance(value, str):
                macro += '="' + value + '"'
            elif isinstance(value, int):
                macro += '=' + str(value)
            else:
                msg = message('warning', colors.WARNING, 'unsupported macro type ' + str(type(value)))
                print(msg)

                macro += '=' + str(value)

            cmd_compile.append(macro)

        command_map[source] = (output, cmd_compile)

    # Linking command
    output = None
    if build.linking:
        all_targets_dir = os.path.join(smake_dir, 'targets')
        os.makedirs(all_targets_dir, exist_ok=True)

        output = os.path.join(all_targets_dir, build.exec)

        cmd_link = [build.compiler, '-o', output]
        cmd_link += build_outputs
        cmd_link += ['-l' + lib for lib in build.libraries]

        command_map['__link__'] = (output, cmd_link)

    return command_map, output

# Check if a file needs to be recompiled
def needs_recompile(build, source, source_out_mtime, dependency_mtime):
    for index in build._cc_sources[source]:
        if dependency_mtime[index] > source_out_mtime:
            return True
    
    return False

def __compile_thread_init__(_counter, _last, _total):
    global counter, last, total

    counter = _counter
    last = _last
    total = _total

def compile_source(build, dependency_mtime, source, output, cmd_compile, index):
    global counter, last, total

    source_out_mtime = 0
    if os.path.exists(output):
        source_out_mtime = os.path.getmtime(output)

    recompile = needs_recompile(build, source, source_out_mtime, dependency_mtime)

    if not recompile:
        with counter.get_lock():
            size = shutil.get_terminal_size()
            print(' ' * size.columns, end='\r')
            msg = colors.BOLD + f'[{counter.value}/{total}]' + colors.RESET
            msg += f' {source} '
            msg += colors.OKBLUE + '(up to date)' + colors.RESET
            print(msg, end='\r')
            counter.value += 1

        with last.get_lock():
            last.value = index

        return source

    proc = subprocess.Popen(cmd_compile, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    proc.wait()
    out, err = proc.communicate()

    with counter.get_lock():
        out = out.decode('utf-8')
        err = err.decode('utf-8')

        spaced = False
        if out != None and len(out) > 0:
            print('\n')
            spaced = True
            print(out)

        if err != None and len(err) > 0:
            if not spaced:
                print('\n')
            print(err)

        if proc.returncode != 0:
            size = shutil.get_terminal_size()
            print(' ' * size.columns, end='\r')
            msg = colors.BOLD + f'[{counter.value}/{total}]' + colors.RESET
            msg += f' {source} '
            msg += colors.FAIL + '(failed)' + colors.RESET
            print(msg)
            counter.value += 1

            with last.get_lock():
                last.value = index

            return None

        size = shutil.get_terminal_size()
        print(' ' * size.columns, end='\r')
        msg = colors.BOLD + f'[{counter.value}/{total}]' + colors.RESET
        msg += f' {source} '
        msg += colors.OKGREEN + '(success)' + colors.RESET
        print(msg, end='\r')
        counter.value += 1

        with last.get_lock():
            last.value = index

        return source

def updated_sources(build, cached_time):
    updated = []
    for source in build._cc_sources:
        source_mtime = os.path.getmtime(source)
        if source_mtime > cached_time:
            updated.append(source)

    return updated

def update_dependencies(build, updated_sources, threads):
    # First prune old dependencies and reindex
    dependencies = build._cc_dependencies

    pruned_dependencies = []
    pruned_sources = {}

    for source in build._cc_sources:
        if source in updated_sources:
            continue

        deps = build._cc_sources[source]

        indices = []
        for dep in deps:
            dep = dependencies[dep]
            if dep in pruned_sources:
                indices.append(pruned_dependencies.index(dep))
            else:
                indices.append(dependencies.index(dep))
                pruned_dependencies.append(dep)

        pruned_sources[source] = indices

    # Generate the commands for generating dependencies
    cc_dep_commands = []
    for source in updated_sources:
        cmd = [build.compiler, '-MM', source]
        cmd.extend(build._cc_includes)
        cc_dep_commands.append((source, cmd))

    # Concurrently generate dependencies
    counter = multiprocessing.Value('i', 1)
    with multiprocessing.Pool(threads,
            initializer=__dependency_thread_init__,
            initargs=(counter, len(cc_dep_commands))) as pool:
        rets = pool.map(get_dependencies, cc_dep_commands)

    for i in range(len(updated_sources)):
        source = updated_sources[i]
        deps = rets[i]

        pruned_sources[source] = []
        for dep in deps:
            dep = dep.strip()
            if dep in ['', '\\', ':']:
                continue

            if dep in pruned_dependencies:
                pruned_sources[source].append(pruned_dependencies.index(dep))
            else:
                pruned_sources[source].append(len(pruned_dependencies))
                pruned_dependencies.append(dep)

    # Update the build object
    build._cc_dependencies = pruned_dependencies
    build._cc_sources = pruned_sources

def do_build(build, threads):
    for req in build.requires:
        msg = f'Building dependency {req.code}...'
        print(message('info', colors.OKBLUE, msg))
        ret = do_build(req, threads)
        if ret is None:
            return None

    # Check if we can use the cache
    build_dir = os.path.dirname(build.path)
    smake_dir = os.path.join(build_dir, '.smake')
    cache_dir = os.path.join(smake_dir, 'serialized')
    cache_file = os.path.join(cache_dir, build.code + '.smake.cache')

    cache_success = False
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            cache = pickle.load(f)
            if cache == build:
                # TODO: method
                cached_time = os.path.getmtime(cache_file)
                updated = updated_sources(cache, cached_time)
                if len(updated) > 0:
                    update_dependencies(cache, updated, threads)
                cache_success = True

                # NOTE: there are some fields that have to transfered from
                # parsing which only apply during the compilation step
                # TODO: method
                cache.linking = build.linking
                cache.object_format = build.object_format
                cache.object_dir = build.object_dir

                build = cache
            # TODO: shouldnt we be checking this in either case?
            elif not cache.flags == build.flags:
                # TODO: method to check all conditions where we have to clear
                # cached objects
                all_builds_dir = os.path.join(smake_dir, 'builds')
                build_dir = os.path.join(all_builds_dir, build.code)
                shutil.rmtree(build_dir, ignore_errors=True)
            elif not cache.macros == build.macros:
                all_builds_dir = os.path.join(smake_dir, 'builds')
                build_dir = os.path.join(all_builds_dir, build.code)
                shutil.rmtree(build_dir, ignore_errors=True)

    # TODO: decouple from this method
    if not cache_success:
        ret = preprocess_build(build, threads)
        if ret != None:
            return None

    # Cache the build object
    # TODO: method
    os.makedirs(cache_dir, exist_ok=True)
    file = os.path.join(cache_dir, build.code + '.smake.cache')
    with open(file, 'wb') as f:
        pickle.dump(build, f)

    # Generate commands for compiling
    command_map, output = generate_commands(build)

    dependency_mtime = get_dependency_mtime(build._cc_dependencies)

    args = []

    source_list = list(command_map.keys())
    for source in command_map:
        if source == '__link__':
            continue

        output, cmd_compile = command_map[source]
        index = source_list.index(source)
        args.append((build, dependency_mtime, source, output, cmd_compile, index))

    counter = multiprocessing.Value('i', 1)
    last = multiprocessing.Value('i', -1)
    with multiprocessing.Pool(threads,
                initializer=__compile_thread_init__,
                initargs=(counter, last, len(args))
            ) as pool:
        rets = pool.starmap(compile_source, args)

    # Gather failed sources
    # TODO: method
    source_indexed = command_map.copy()
    # source_indexed.pop('__link__')
    source_indexed = list(source_indexed.keys())

    failures = []
    for i in range(len(rets)):
        if rets[i] == None:
            failures.append(source_indexed[i])

    last_source = source_list[last.value]
    if len(failures) > 0:
        size = shutil.get_terminal_size()
        if last_source in failures:
            print()
        else:
            print('\n')

        print(' ' * size.columns, end='\r')
        msg = 'Failed to compile the following sources:'
        print(message('error', colors.FAIL, msg))
        for failure in failures:
            print('\t', failure)
        return
    else:
        size = shutil.get_terminal_size()
        print('\n')
        print(' ' * size.columns, end='\r')
        msg = 'All sources compiled successfully for ' + build.code
        print(message('success', colors.OKGREEN, msg))

    # Linking
    # TODO: method
    if build.linking:
        output, cmd_link = command_map['__link__']
        proc = subprocess.Popen(cmd_link, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        proc.wait()
        out, err = proc.communicate()

        if out != None and len(out) > 0:
            print(out.decode('utf-8'))

        if err != None and len(err) > 0:
            print(err.decode('utf-8'))

        if proc.returncode != 0:
            msg = 'Failed to link the target for ' + build.code
            print(message('error', colors.FAIL, msg))
            return
        else:
            msg = 'Successfully linked the target for ' + build.code
            print(message('success', colors.OKGREEN, msg))

        return output

    return 0

argparser = argparse.ArgumentParser()
argparser.add_argument('target',
    help='The target to build',
    default=None,
    nargs='?'
)

argparser.add_argument('-m', "--mode",
    help="Execution mode",
    default='default'
)

argparser.add_argument('-j', "--threads",
    help="Number of threads to use",
    default=os.cpu_count(),
    type=int
)

argparser.add_argument('-l', "--list",
    help="List all targets",
    action='store_true'
)

if __name__ == '__main__':
    args = argparser.parse_args()

    # Load all smake files
    load_smakes()
    targets = Target.all

    if args.list:
        list_targets(targets)
        exit(0)

    if args.target not in targets:
        msg = 'Target ' + args.target + ' not found'
        print(message('error', colors.FAIL, msg))
        exit(1)

    target = targets[args.target]
    if args.mode not in target.builds:
        msg = f'Build args.mode \'{args.mode}\' not found for target \'{target.name}\''
        print(message('error', colors.FAIL, msg))
        exit(1)

    build = target.builds[args.mode]
    output = do_build(build, args.threads)
    if output == None:
        exit(1)

    # TODO: another upper level method
    if args.mode in target.post_builds:
        script_variables = {
            'SMAKE_TARGET': os.path.abspath(output),
        }

        script = target.post_builds[args.mode]
        if len(script.commands) == 0:
            msg = 'No commands specified in the post-build script'
            print(message('warning', colors.WARNING, msg))
            exit(0)

        for var in script_variables:
            for i in range(len(script.commands)):
                cmd = script.commands[i]
                script.commands[i] = cmd.replace('${' + var + '}', script_variables[var])

        msg = 'Running post-build script for target ' + target.name + ' in mode ' + args.mode
        print(message('info', colors.OKBLUE, msg))
        for cmd in script.commands:
            print(message('exec', colors.OKCYAN, cmd))
            subprocess.call(cmd, shell=True, cwd=script._cwd)
